\section{Recap: Score Function Estimator}

\frame{\tableofcontents[currentsection]}

\begin{frame}{Score Function Estimator}

We are interested in computing
\vspace{-5pt}
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q_\lambda(z|x)]{\log p_\theta(x|z)} \\  \pause
%&= \pdv{\lambda} \sum_z q(z|x,\lambda)\log p(x|z, \theta)  \\ 
&=  \sum_z \alert{\pdv{\lambda}(q(z|x, \lambda))} \log p(x|z, \theta) \\  \pause
&= \sum_z \textcolor{blue}{q(z|x, \lambda) \pdv{\lambda}(\log q(z|x, \lambda))} \log p(x|z, \theta)  \\ \pause
&= \underbrace{\mathbb E_{q(z|x, \lambda)} \left[  \log p(x|z, \theta)  \pdv{\lambda}\log q(z|x, \lambda)\right]}_{\text{expected gradient :)}}
\end{aligned}
\end{equation*}


\end{frame}

\begin{frame}{Score Function Estimator}

We can now build an MC estimator
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)} \\ 
&= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right] \\ \pause 
&\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \alert{\log p(x|z^{(s)}, \theta)} \pdv{\lambda}\log q(z^{(s)}|x, \lambda) \\
&\textcolor{gray}{\text{where } z^{(s)} \sim q(z|x, \lambda)}
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Score Function Estimator: Variance}
\begin{small}
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)}
= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right]
\end{aligned}
\end{equation*}
\end{small}


Empirically this estimator often exhibits high variance.

~

\pause 

\begin{itemize}
	\item the magnitude of $\log p(x|z, \theta)$ varies widely \pause 
	\item the model likelihood does not contribute to direction of gradient 
\end{itemize}
\end{frame}


\begin{frame}{Score Function Estimator: Variance}
Idea: standardize the "reward" $r(z) \defeq \log p(x|z, \theta)$ to have a mean at 0 and a variance of 1 \pause

~ 

\begin{itemize}
  \setlength\itemsep{0.7em}
    \item Keep a moving average of the mean and variance $\log p(x|z, \theta)$: $\hat{\mu}$ and $\hat{\sigma^2}$. \pause
    \item $\hat{r}(z) = \frac{\log p(x|z, \theta)-\hat{\mu}}{\sqrt{\hat{\sigma^2}}}$
\end{itemize}

\pause

\begin{small}
\begin{equation*}
\begin{aligned}
\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)}
&= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right]\\ \pause
&\approx E_{q(z|x,\lambda)} \left[  \hat{r}(z) \pdv{\lambda}\log q(z|x,\lambda)\right]
\end{aligned}
\end{equation*}
\end{small}

\end{frame}

\begin{frame}{Score Function Estimator: Variance}

\begin{itemize}
    \item We can show that using these \emph{baselines} do not bias the estimator. \pause
    \item More generally, we can design more sophisticated \emph{control variates} that further reduce the variance.
\end{itemize}

\end{frame}

\section{Control Variates and Baselines}

\frame{\tableofcontents[currentsection]}

\begin{frame}{Control variates}

\begin{alertblock}{Intuition}
To estimate $\mathbb E[f(z)]$ via Monte Carlo we compute the empirical average of $\hat f(z)$ where $\hat f(z)$ is chosen so that $\mathbb E[\hat f(z)] = \mathbb E[f(z)]$ and $\Var(f) > \Var(\hat f)$.
\end{alertblock}

\end{frame}



\begin{frame}{Equivalent expectations}

Let $\bar f = \mathbb E[f(z)]$ be an expectation of interest \pause
\begin{itemize}
	\item say we know $\bar c = \mathbb E[c(z)]$ \pause
	\item then for $\hat f(z) \triangleq f(z) - b(c(z) - \mathbb E[c(z)])$ \\ \pause
	it holds that $\mathbb E[\hat f(z)] = \mathbb E[f(z)]$ \pause
	\item and $\Var(\hat f) = \Var(f) - 2b \Cov(f, c) + b^2 \Var(c)$ 
\end{itemize}

\end{frame}

\begin{frame}{Choosing the control variate}

\begin{enumerate}
	\item $\hat f(z) \triangleq f(z) - b(c(z) - \mathbb E[c(z)])$ 
	\item $\Var(\hat f) = \Var(f) - 2b \Cov(f, c) + b^2 \Var(c)$ 
\end{enumerate}

How do we choose $b$ and $c(z)$? \pause
\begin{itemize}
	\item If $f(z)$ and $c(z)$ are positively correlated, then we may reduce variance \pause
	\item solving $\pdv{b}\Var(\hat f) = 0$ \pause yields $b^\star = \sfrac{\Cov(f,c)}{\Var(c)}$
\end{itemize}

\pause

Of course, $\mathbb E[c(z)]$ must be known!

\end{frame}

\begin{frame}{MC}
We then use the estimate
\begin{equation*}
\bar f \overset{\text{MC}}{\approx} \frac{1}{S} \left( \sum_{s=1}^S f(z^{(s)}) - b c(z^{(s)})\right) + b \bar c
\end{equation*}
\pause

And recall that for us 
\begin{equation*}
f(z) = \log p(x|z, \theta) \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
and $z^{(s)} \sim q(z|x, \lambda)$
\end{frame}


\begin{frame}{Expected score}

The Expectation of the score function is 0. 
\begin{equation*}
\begin{aligned}
&\E[q(z|x,\lambda)]{\pdv{\lambda} \log q(z|x,\lambda)} \\ \pause
&= \int q(z|x, \lambda) \pdv{\lambda} \log q(z|x, \lambda) \dd z\\ \pause
&= \int  \pdv{\lambda} q(z|x, \lambda) \dd z \\ \pause
&= \pdv{\lambda} \int q(z|x, \lambda) \dd z \\ \pause
&= \pdv{\lambda} 1 = \textcolor{blue}{0} 
\end{aligned}
\end{equation*}

\end{frame}

\begin{frame}{Baselines}
With
\begin{equation*}
f(z) = \log p(x|z, \theta) \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
and 
\begin{equation*}
c(z) = \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
we have 
\begin{equation*}
\hat f(z) = \pause (\log p(x|z, \theta) - b) \pdv{\lambda} \log q(z|x, \lambda) 
\end{equation*}

\pause
$b$ is known as \emph{baseline} in RL literature.
\end{frame}

\begin{frame}{Examples of baselines}

\begin{itemize}
	\item Moving average of $\log p(x|z, \theta)$ \\
	 based on previous batches \pause
	\item A trainable constant $b$ \pause
	\item A neural network prediction based on $x$\\
	e.g. $b(x; \omega)$ \pause
	\item The likelihood assessed at a deterministic point, e.g. \\
	$b(x) = \log p(x|z^\star, \theta)$ where $z^\star = \argmax_z q(z|x, \lambda)$
\end{itemize}
\end{frame}


\begin{frame}{Trainable baselines}
Baselines are predicted by a regression model (e.g. a neural net). \\

~

The model is trained using 
an $ L_{2} $-loss.
\begin{equation*}
\min_\omega \left(b(x; \omega) - \log p(x|z,\theta)\right)^{2}
\end{equation*}
\end{frame}

\begin{frame}{Summary}
    
\begin{itemize}
    \item The score function estimator inhibits high variance. \pause
    \item We can design control variates that reduce estimator variance, yet do not bias the estimator!
\end{itemize}

\end{frame}



\begin{frame}[allowframebreaks]{Literature}
%\nocite{KingmaWelling:2013}
% \nocite{HintonEtAl:1995}
% \nocite{MnihNVIL}
 \nocite{PaisleyEtAl:2012}
%  \nocite{RanganathEtAl:2014}
 \nocite{Williams:1992}
%  \nocite{greensmith2004variance} 
%  \nocite{gu2015muprop}
%  \nocite{tucker2017rebar}
%  \nocite{grathwohl2017backpropagation}


\bibliographystyle{unsrtnat}
\bibliography{../../VI}
\end{frame}
