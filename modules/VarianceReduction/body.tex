\section{Recap: Score Function Estimator}

\frame{\tableofcontents[currentsection]}

\begin{frame}{Score Function Estimator}

We are interested in computing
\vspace{-5pt}
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q_\lambda(z|x)]{\log p_\theta(x|z)} \\  \pause
%&= \pdv{\lambda} \sum_z q(z|x,\lambda)\log p(x|z, \theta)  \\ 
&=  \sum_z \alert{\pdv{\lambda}(q(z|x, \lambda))} \log p(x|z, \theta) \\  \pause
&= \sum_z \textcolor{blue}{q(z|x, \lambda) \pdv{\lambda}(\log q(z|x, \lambda))} \log p(x|z, \theta)  \\ \pause
&= \underbrace{\mathbb E_{q(z|x, \lambda)} \left[  \log p(x|z, \theta)  \pdv{\lambda}\log q(z|x, \lambda)\right]}_{\text{expected gradient :)}}
\end{aligned}
\end{equation*}


\end{frame}

\begin{frame}{Score Function Estimator}

We can now build an MC estimator
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)} \\ 
&= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right] \\ \pause 
&\overset{\text{MC}}{\approx} \frac{1}{S} \sum_{s=1}^S \alert{\log p(x|z^{(s)}, \theta)} \pdv{\lambda}\log q(z^{(s)}|x, \lambda) \\
&\textcolor{gray}{\text{where } z^{(s)} \sim q(z|x, \lambda)}
\end{aligned}
\end{equation*}
\end{frame}

\begin{frame}{Score Function Estimator: Variance}
\begin{small}
\begin{equation*}
\begin{aligned}
&\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)}
= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right]
\end{aligned}
\end{equation*}
\end{small}


Empirically this estimator often exhibits high variance.

~

\pause 

\begin{itemize}
	\item the magnitude of $\log p(x|z, \theta)$ varies widely \pause 
	\item the model likelihood does not contribute to direction of gradient 
\end{itemize}
\end{frame}


\begin{frame}{Score Function Estimator: Variance}
Idea: standardize the "reward" $r(z) \defeq \log p(x|z, \theta)$ to have a mean at 0 and a variance of 1 \pause

~ 

\begin{itemize}
  \setlength\itemsep{0.7em}
    \item Keep a moving average of the mean and variance $\log p(x|z, \theta)$: $\hat{\mu}$ and $\hat{\sigma^2}$. \pause
    \item $\hat{r}(z) = \frac{\log p(x|z, \theta)-\hat{\mu}}{\sqrt{\hat{\sigma^2}}}$
\end{itemize}

\pause

\begin{small}
\begin{equation*}
\begin{aligned}
\pdv{\lambda}\E[q(z|x,\lambda)]{\log p(x|z,\theta)}
&= \mathbb E_{q(z|x,\lambda)} \left[  \log p(x|z,\theta)  \pdv{\lambda}\log q(z|x,\lambda)\right]\\ \pause
&\approx E_{q(z|x,\lambda)} \left[  \hat{r}(z) \pdv{\lambda}\log q(z|x,\lambda)\right]
\end{aligned}
\end{equation*}
\end{small}

\end{frame}

\begin{frame}{Score Function Estimator: Variance}

\begin{itemize}
    \item We can show that using these \emph{baselines} do not bias the estimator. \pause
    \item More generally, we can design more sophisticated \emph{control variates} that further reduce the variance.
\end{itemize}

\end{frame}

\section{Control Variates and Baselines}

\frame{\tableofcontents[currentsection]}

\begin{frame}{Control variates}

\begin{alertblock}{Intuition}
To estimate $\mathbb E[f(z)]$ via Monte Carlo we compute the empirical average of $\hat f(z)$ where $\hat f(z)$ is chosen so that $\mathbb E[\hat f(z)] = \mathbb E[f(z)]$ and $\Var(f) > \Var(\hat f)$.
\end{alertblock}

\end{frame}



\begin{frame}{Equivalent expectations}

Let $\bar f = \mathbb E[f(z)]$ be an expectation of interest \pause
\begin{itemize}
	\item say we know $\bar c = \mathbb E[c(z)]$ \pause
	\item then for $\hat f(z) \triangleq f(z) - b(c(z) - \mathbb E[c(z)])$ \\ \pause
	it holds that $\mathbb E[\hat f(z)] = \mathbb E[f(z)]$ \pause
	\item and $\Var(\hat f) = \Var(f) - 2b \Cov(f, c) + b^2 \Var(c)$ 
\end{itemize}

\end{frame}

\begin{frame}{Choosing the control variate}

\begin{enumerate}
	\item $\hat f(z) \triangleq f(z) - b(c(z) - \mathbb E[c(z)])$ 
	\item $\Var(\hat f) = \Var(f) - 2b \Cov(f, c) + b^2 \Var(c)$ 
\end{enumerate}

How do we choose $b$ and $c(z)$? \pause
\begin{itemize}
	\item If $f(z)$ and $c(z)$ are positively correlated, then we may reduce variance \pause
	\item solving $\pdv{b}\Var(\hat f) = 0$ \pause yields $b^\star = \sfrac{\Cov(f,c)}{\Var(c)}$
\end{itemize}

\pause

Of course, $\mathbb E[c(z)]$ must be known!

\end{frame}

\begin{frame}{MC}
We then use the estimate
\begin{equation*}
\bar f \overset{\text{MC}}{\approx} \frac{1}{S} \left( \sum_{s=1}^S f(z^{(s)}) - b c(z^{(s)})\right) + b \bar c
\end{equation*}
\pause

And recall that for us 
\begin{equation*}
f(z) = \log p(x|z, \theta) \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
and $z^{(s)} \sim q(z|x, \lambda)$
\end{frame}


\begin{frame}{Expected score}

The Expectation of the score function is 0. 
\begin{equation*}
\begin{aligned}
&\E[q(z|x,\lambda)]{\pdv{\lambda} \log q(z|x,\lambda)} \\ \pause
&= \int q(z|x, \lambda) \pdv{\lambda} \log q(z|x, \lambda) \dd z\\ \pause
&= \int  \pdv{\lambda} q(z|x, \lambda) \dd z \\ \pause
&= \pdv{\lambda} \int q(z|x, \lambda) \dd z \\ \pause
&= \pdv{\lambda} 1 = \textcolor{blue}{0} 
\end{aligned}
\end{equation*}

\end{frame}

\begin{frame}{Baselines}
With
\begin{equation*}
f(z) = \log p(x|z, \theta) \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
and 
\begin{equation*}
c(z) = \pdv{\lambda} \log q(z|x, \lambda)
\end{equation*}
we have 
\begin{equation*}
\hat f(z) = \pause (\log p(x|z, \theta) - b) \pdv{\lambda} \log q(z|x, \lambda) 
\end{equation*}

\pause
$b$ is known as \emph{baseline} in RL literature.
\end{frame}

\begin{frame}{Examples of baselines}

\begin{itemize}
	\item Moving average of $\log p(x|z, \theta)$ \\
	 based on previous batches \pause
	\item A trainable constant $b$ \pause
	\item A neural network prediction based on $x$\\
	e.g. $b(x; \omega)$ \pause
	\item The likelihood assessed at a deterministic point, e.g. \\
	$b(x) = \log p(x|z^\star, \theta)$ where $z^\star = \argmax_z q(z|x, \lambda)$
\end{itemize}
\end{frame}


\begin{frame}{Trainable baselines}
Baselines are predicted by a regression model (e.g. a neural net). \\

~

The model is trained using 
an $ L_{2} $-loss.
\begin{equation*}
\min_\omega \left(b(x; \omega) - \log p(x|z,\theta)\right)^{2}
\end{equation*}
\end{frame}

\begin{frame}{Summary}
    
\begin{itemize}
    \item In practice the score function estimator leads to high variance gradient estimates. \pause
    \item We can design control variates that reduce estimator variance, yet do not bias the estimator!
\end{itemize}

\end{frame}



\begin{frame}[allowframebreaks]{Literature}
%\nocite{KingmaWelling:2013}
% \nocite{HintonEtAl:1995}
% \nocite{MnihNVIL}
 \nocite{PaisleyEtAl:2012}
%  \nocite{RanganathEtAl:2014}
 \nocite{Williams:1992}
%  \nocite{greensmith2004variance} 
%  \nocite{gu2015muprop}
%  \nocite{tucker2017rebar}
%  \nocite{grathwohl2017backpropagation}


\bibliographystyle{unsrtnat}
\bibliography{../../VI}
\end{frame}
